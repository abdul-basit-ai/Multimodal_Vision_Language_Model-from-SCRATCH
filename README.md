# Multimodal_Vision_Language_Model-from-SCRATCH
### For detailed notes, check out the pdf

I implemented Google's Paligemma Vision language model, using siglip image encoder and gemma 2 language model.
PaliGemma is a lightweight open vision-language model designed for tasks like image captioning, visual question answering, object detection, and image segmentation. This implementation provides the core architecture and inference capabilities.

## Features

- Full model architecture with SigLIP vision encoder and Gemma decoder
- Support for multiple vision-language tasks
- Efficient image-text processing pipeline
- Pre-trained model loading and inference

## Model Architecture

- **Vision Encoder**: SigLIP-So400m (transforms images into visual tokens)
- **Language Model**: Gemma 2B/3B (processes text and visual tokens)
- **Resolution**: 224x224 image input


<img width="1106" height="767" alt="image" src="https://github.com/user-attachments/assets/1953bf42-2fd7-4af3-8dae-6b1d77de71be" />

<img width="598" height="846" alt="image" src="https://github.com/user-attachments/assets/06b84859-22da-4ee7-8a20-2696a1ed6453" />
